{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpx1JAesULnN"
      },
      "source": [
        "# Setup:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKFRRB8qT9ED",
        "outputId": "0dd715dd-880d-4613-899f-f3f1832a7b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Casey Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ECE 661/Final Project')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MZVUD6-wUrnF"
      },
      "outputs": [],
      "source": [
        "# Dependencies\n",
        "import argparse\n",
        "import os, sys\n",
        "import time\n",
        "import datetime\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#from tools.dataset import CIFAR10\n",
        "#from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHe82gKxiSnH"
      },
      "source": [
        "# ResNet Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rf2LOd_iRYV",
        "outputId": "c8a4912d-ce90-4c05-8576-ed25e2a5c129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet20\n",
            "Total number of params 269722\n",
            "Total layers 20\n",
            "\n",
            "resnet32\n",
            "Total number of params 464154\n",
            "Total layers 32\n",
            "\n",
            "resnet44\n",
            "Total number of params 658586\n",
            "Total layers 44\n",
            "\n",
            "resnet56\n",
            "Total number of params 853018\n",
            "Total layers 56\n",
            "\n",
            "resnet110\n",
            "Total number of params 1727962\n",
            "Total layers 110\n",
            "\n",
            "resnet1202\n",
            "Total number of params 19421274\n",
            "Total layers 1202\n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Properly implemented ResNet-s for CIFAR10 as described in paper [1].\n",
        "The implementation and structure of this file is hugely influenced by [2]\n",
        "which is implemented for ImageNet and doesn't have option A for identity.\n",
        "Moreover, most of the implementations on the web is copy-paste from\n",
        "torchvision's resnet and has wrong number of params.\n",
        "Proper ResNet-s for CIFAR10 (for fair comparision and etc.) has following\n",
        "number of layers and parameters:\n",
        "name      | layers | params\n",
        "ResNet20  |    20  | 0.27M\n",
        "ResNet32  |    32  | 0.46M\n",
        "ResNet44  |    44  | 0.66M\n",
        "ResNet56  |    56  | 0.85M\n",
        "ResNet110 |   110  |  1.7M\n",
        "ResNet1202|  1202  | 19.4m\n",
        "which this implementation indeed has.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "[2] https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
        "If you use this implementation in you work, please don't forget to mention the\n",
        "author, Yerlan Idelbayev.\n",
        "'''\n",
        "\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7MPUvV5aLVX"
      },
      "source": [
        "# RotNet (Do not change any of the below for SimCLR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "e56dc15573c446b6ac8c3e2dfddf619c",
            "73a99ccd039c4288947bdb7f2e6e76d0",
            "154d79d59e944a88874c128d11d82916",
            "3bc62df8b4b140a3a5f49977495fa6aa",
            "ede04e760e2446a59d7dd0a74a803182",
            "6a6a99f944034a9eab12d99a4b9368c6",
            "706a00ed84ef49b0b29893369cce5f44",
            "e37db0dc5f5f43eca04a53b389edb6ef",
            "67c4fa30285a4ac6a2363f72b4175d89",
            "d9cb320b2aab4d8ca814c5ccd456eeff",
            "fcc9055552324dd0b0585682ff8855be"
          ]
        },
        "id": "Ospo5D1LUnAi",
        "outputId": "b29e7992-0676-479b-f23a-3463b1e930c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e56dc15573c446b6ac8c3e2dfddf619c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Loading in CIFAR Dataset\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=16)\n",
        "val_loader = torch.utils.data.DataLoader(valset, batch_size=100, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yolGxdedtAIa"
      },
      "outputs": [],
      "source": [
        "# Convert the batch into 4 rotated images\n",
        "\n",
        "def rotnet_conversion(batch):\n",
        "  size = batch.size()\n",
        "  new_batch = torch.empty((0, size[1], size[2], size[3]))\n",
        "  new_labels = torch.LongTensor([])\n",
        "  for idx in range(size[0]):\n",
        "    original_img = batch[idx]\n",
        "    rot_0 = original_img\n",
        "    rot_90 = transforms.functional.rotate(original_img, 90)\n",
        "    rot_180 = transforms.functional.rotate(original_img, 180)\n",
        "    rot_270 = transforms.functional.rotate(original_img, 270)\n",
        "    rot_images = torch.stack((rot_0, rot_90, rot_180, rot_270), 0)\n",
        "    rot_labels = torch.LongTensor([0,1,2,3])\n",
        "\n",
        "    new_batch = torch.cat((new_batch, rot_images))\n",
        "    new_labels = torch.cat((new_labels, rot_labels))\n",
        "  return new_batch, new_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ICT9X0XVsMT"
      },
      "outputs": [],
      "source": [
        "# RotNet Setup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "net = ResNet(BasicBlock, [3, 3, 3], num_classes = 4)\n",
        "net.to(device)\n",
        "\n",
        "# Parameters (as described in the paper)\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr = INITIAL_LR, momentum = MOMENTUM, weight_decay = REG)\n",
        "EPOCHS = 100\n",
        "DECAY_EPOCHS = {30, 60, 80}\n",
        "DECAY = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i16vO98WeAb",
        "outputId": "c3ef705c-bdd7-42d3-f10a-71e2f25bbda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "Training loss: 1.0009, Training accuracy: 0.5664\n",
            "Validation loss: 0.8996, Validation accuracy: 0.6251\n",
            "Saving ...\n",
            "\n",
            "Epoch 1:\n",
            "Training loss: 0.8178, Training accuracy: 0.6652\n",
            "Validation loss: 0.8420, Validation accuracy: 0.6599\n",
            "Saving ...\n",
            "\n",
            "Epoch 2:\n",
            "Training loss: 0.6989, Training accuracy: 0.7193\n",
            "Validation loss: 0.7149, Validation accuracy: 0.7163\n",
            "Saving ...\n",
            "\n",
            "Epoch 3:\n",
            "Training loss: 0.5985, Training accuracy: 0.7648\n",
            "Validation loss: 0.6251, Validation accuracy: 0.7573\n",
            "Saving ...\n",
            "\n",
            "Epoch 4:\n",
            "Training loss: 0.5395, Training accuracy: 0.7917\n",
            "Validation loss: 0.5592, Validation accuracy: 0.7809\n",
            "Saving ...\n",
            "\n",
            "Epoch 5:\n",
            "Training loss: 0.4960, Training accuracy: 0.8092\n",
            "Validation loss: 0.6103, Validation accuracy: 0.7615\n",
            "\n",
            "Epoch 6:\n",
            "Training loss: 0.4662, Training accuracy: 0.8222\n"
          ]
        }
      ],
      "source": [
        "# Training the RotNet with rotated images\n",
        "CHECKPOINT_FOLDER = \"./content/drive/MyDrive/ECE 661/Final Project\"\n",
        "\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    # handle the learning rate scheduler.\n",
        "    if i in DECAY_EPOCHS:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "    \n",
        "    # Training\n",
        "    net.train()\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0\n",
        "    \n",
        "    print(\"Epoch %d:\" %i)\n",
        "    \n",
        "    # One Epoch\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "      \n",
        "        inputs, targets = rotnet_conversion(inputs)\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        out = net.forward(inputs)\n",
        "        loss = criterion(out, targets)\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Counting Correct Predictions\n",
        "        for idx, val in enumerate(out):\n",
        "          max_out_idx = torch.argmax(val)\n",
        "          target_idx = targets[idx]\n",
        "\n",
        "          if max_out_idx == target_idx:\n",
        "            correct_examples += 1\n",
        "          \n",
        "          total_examples += 1\n",
        "\n",
        "\n",
        "    # Compute Loss/Accuracy     \n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "\n",
        "    # Validation\n",
        "    net.eval()\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "\n",
        "            inputs, targets = rotnet_conversion(inputs)\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            out = net.forward(inputs)\n",
        "            loss = criterion(out, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # Counting Correct Predictions\n",
        "            for idx, val in enumerate(out):\n",
        "              max_out_idx = torch.argmax(val)\n",
        "              target_idx = targets[idx]\n",
        "\n",
        "              if max_out_idx == target_idx:\n",
        "                correct_examples += 1\n",
        "              \n",
        "              total_examples += 1\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    \n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "            os.makedirs(CHECKPOINT_FOLDER)\n",
        "        print(\"Saving ...\")\n",
        "        state = {'state_dict': net.state_dict(),\n",
        "                 'epoch': i,\n",
        "                 'lr': current_learning_rate}\n",
        "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'RotNet.pth'))\n",
        "        \n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training RotNet model on original CIFAR task"
      ],
      "metadata": {
        "id": "ktEZ97dxoO-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(layer):\n",
        "  init.kaiming_normal_(layer.weight)"
      ],
      "metadata": {
        "id": "HVasXcX1T0JC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotNet_Add_Linear_Layer(nn.Module):\n",
        "    # rotnet: The network trained on image rotation\n",
        "    def __init__(self, rotnet):\n",
        "        super(RotNet_Add_Linear_Layer, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = rotnet.conv1\n",
        "        self.bn1 = rotnet.bn1\n",
        "        self.layer1 = rotnet.layer1\n",
        "        self.layer2 = rotnet.layer2\n",
        "        self.layer3 = rotnet.layer3\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "\n",
        "        initialize_weights(self.linear)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "wMEmUpnMfh_a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotNet_Linear_Layer_After_Second_Conv(nn.Module):\n",
        "    # rotnet: The network trained on image rotation\n",
        "    def __init__(self, rotnet):\n",
        "        super(RotNet_Linear_Layer_After_Second_Conv, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = rotnet.conv1\n",
        "        self.bn1 = rotnet.bn1\n",
        "        self.layer1 = rotnet.layer1\n",
        "        self.layer2 = rotnet.layer2\n",
        "        self.linear = nn.Linear(8192, 10)\n",
        "\n",
        "        initialize_weights(self.linear)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pPKqK4kIEunA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RotNet_Conv_And_Linear_Layer_After_Second_Conv(nn.Module):\n",
        "    # rotnet: The network trained on image rotation\n",
        "    def __init__(self, rotnet, layer3):\n",
        "        super(RotNet_Conv_And_Linear_Layer_After_Second_Conv, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = rotnet.conv1\n",
        "        self.bn1 = rotnet.bn1\n",
        "        self.layer1 = rotnet.layer1\n",
        "        self.layer2 = rotnet.layer2\n",
        "        self.layer3 = layer3\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "\n",
        "        initialize_weights(self.linear)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rs7fsFHxLBRI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading in Rotnet model\n",
        "RotNet_path = \"/content/drive/MyDrive/ECE 661/Final Project/RotNetFinal.pth\"\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "rotnet = ResNet(BasicBlock, [3, 3, 3], num_classes = 4)\n",
        "rotnet.load_state_dict(torch.load(RotNet_path)[\"state_dict\"])\n",
        "torch.manual_seed(2022)\n",
        "\n",
        "# Parameters\n",
        "INITIAL_LR = 0.1\n",
        "MOMENTUM = 0.9\n",
        "REG = 5e-4\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "EPOCHS = 50\n",
        "DECAY_EPOCHS = {15, 30, 40}\n",
        "DECAY = 0.2"
      ],
      "metadata": {
        "id": "ajefgex7egZ4"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default\n",
        "net = ResNet(BasicBlock, [3, 3, 3], num_classes = 10)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "waaSzhpL5JS3"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the final linear layer with a new one\n",
        "net = RotNet_Add_Linear_Layer(rotnet)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "Cw3Zoyqh0V-t"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the final conv block, and put linear layer after second conv block\n",
        "net = RotNet_Linear_Layer_After_Second_Conv(rotnet)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "O6MPxFcpBGdd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the final conv block/final linear layer with a new final conv block/linear layer\n",
        "dummy = ResNet(BasicBlock, [3, 3, 3], num_classes = 4)\n",
        "net = RotNet_Conv_And_Linear_Layer_After_Second_Conv(rotnet, dummy.layer3)\n",
        "net = net.to(device)"
      ],
      "metadata": {
        "id": "Ddr-o1uYBNgN"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use these optimizers to select specific layers to update\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr = INITIAL_LR, momentum = MOMENTUM, weight_decay = REG)\n",
        "#optimizer = optim.SGD(net.linear.parameters(), lr = INITIAL_LR, momentum = MOMENTUM, weight_decay = REG)\n",
        "\n",
        "#params = list(net.linear.parameters()) + list(net.layer3.parameters())\n",
        "#optimizer = optim.SGD(params, lr = INITIAL_LR, momentum = MOMENTUM, weight_decay = REG)"
      ],
      "metadata": {
        "id": "pLnH-rakGI7T"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semi-Supervised\n",
        "\n",
        "# Taking Subsets of CIFAR Dataset for Semi-Supervised Training\n",
        "# Creates a subset with the specified number of data points from each class\n",
        "\n",
        "def get_subset_trainloader(trainset, num):\n",
        "  train_indices = torch.tensor([])\n",
        "\n",
        "  # Randomly select num samples from each class, and append to tensor of all indices\n",
        "  for i in range(10):\n",
        "    curr_class_labels = torch.tensor(trainset.targets) == i\n",
        "    indices = curr_class_labels.nonzero().reshape(-1)\n",
        "    indices_of_selected_indices = torch.randperm(len(indices))[:num]\n",
        "    selected_indices = indices[indices_of_selected_indices]\n",
        "    train_indices = torch.cat((train_indices, selected_indices))\n",
        "  \n",
        "  # Create data subset using indices, and create train_loader from this subset\n",
        "  train_indices = train_indices.tolist()\n",
        "  train_indices = [int(x) for x in train_indices]\n",
        "  data_subset = torch.utils.data.Subset(trainset, train_indices)\n",
        "  train_loader = torch.utils.data.DataLoader(data_subset, batch_size=128, shuffle=True, num_workers=16)\n",
        "  return train_loader\n",
        "\n",
        "# Note that 5000 is all of the images from each class\n",
        "train_loader = get_subset_trainloader(trainset, 50)"
      ],
      "metadata": {
        "id": "wPt0awXYoKZC"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT_FOLDER = \"./content/drive/MyDrive/ECE 661/Final Project\"\n",
        "\n",
        "best_val_acc = 0\n",
        "current_learning_rate = INITIAL_LR\n",
        "\n",
        "print(\"==> Training starts!\")\n",
        "print(\"=\"*50)\n",
        "for i in range(0, EPOCHS):\n",
        "    # handle the learning rate scheduler.\n",
        "    if i in DECAY_EPOCHS:\n",
        "        current_learning_rate = current_learning_rate * DECAY\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
        "    \n",
        "    # Training\n",
        "    net.train()\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    train_loss = 0\n",
        "\n",
        "    print(\"Epoch %d:\" %i)\n",
        "    \n",
        "    # One Epoch\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "      \n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        out = net.forward(inputs)\n",
        "        loss = criterion(out, targets)\n",
        "        net.zero_grad()\n",
        "        loss.backward()\n",
        "        train_loss += loss\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Counting Correct Predictions\n",
        "        for idx, val in enumerate(out):\n",
        "          max_out_idx = torch.argmax(val)\n",
        "          target_idx = targets[idx]\n",
        "\n",
        "          if max_out_idx == target_idx:\n",
        "            correct_examples += 1\n",
        "          \n",
        "          total_examples += 1\n",
        "\n",
        "    print(total_examples)\n",
        "    # Compute Loss/Accuracy     \n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
        "    \n",
        "\n",
        "\n",
        "    # Validation\n",
        "    net.eval()\n",
        "    total_examples = 0\n",
        "    correct_examples = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    # disable gradient during validation, which can save GPU memory\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            out = net.forward(inputs)\n",
        "            loss = criterion(out, targets)\n",
        "            val_loss += loss\n",
        "\n",
        "            # Counting Correct Predictions\n",
        "            for idx, val in enumerate(out):\n",
        "              max_out_idx = torch.argmax(val)\n",
        "              target_idx = targets[idx]\n",
        "\n",
        "              if max_out_idx == target_idx:\n",
        "                correct_examples += 1\n",
        "              \n",
        "              total_examples += 1\n",
        "\n",
        "    avg_loss = val_loss / len(val_loader)\n",
        "    avg_acc = correct_examples / total_examples\n",
        "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
        "    \n",
        "    # save the model checkpoint\n",
        "    if avg_acc > best_val_acc:\n",
        "        best_val_acc = avg_acc\n",
        "        if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "            os.makedirs(CHECKPOINT_FOLDER)\n",
        "        print(\"Saving ...\")\n",
        "        state = {'state_dict': net.state_dict(),\n",
        "                 'epoch': i,\n",
        "                 'lr': current_learning_rate}\n",
        "        torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'RotNet_Linear_NoFT.pth'))\n",
        "        \n",
        "    print('')\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "z9gDNTrdoWGA",
        "outputId": "7174fc00-c6c8-4dd3-edf6-753a51d9b884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training starts!\n",
            "==================================================\n",
            "Epoch 0:\n",
            "500\n",
            "Training loss: 2.0527, Training accuracy: 0.2380\n",
            "Validation loss: 3.1243, Validation accuracy: 0.2495\n",
            "Saving ...\n",
            "\n",
            "Epoch 1:\n",
            "500\n",
            "Training loss: 1.3174, Training accuracy: 0.5820\n",
            "Validation loss: 6.2897, Validation accuracy: 0.2221\n",
            "\n",
            "Epoch 2:\n",
            "500\n",
            "Training loss: 0.9089, Training accuracy: 0.7180\n",
            "Validation loss: 3.7581, Validation accuracy: 0.2808\n",
            "Saving ...\n",
            "\n",
            "Epoch 3:\n",
            "500\n",
            "Training loss: 0.6171, Training accuracy: 0.8120\n",
            "Validation loss: 18.3939, Validation accuracy: 0.1174\n",
            "\n",
            "Epoch 4:\n",
            "500\n",
            "Training loss: 0.3392, Training accuracy: 0.9400\n",
            "Validation loss: 5.2641, Validation accuracy: 0.2280\n",
            "\n",
            "Epoch 5:\n",
            "500\n",
            "Training loss: 0.2233, Training accuracy: 0.9580\n",
            "Validation loss: 3.8111, Validation accuracy: 0.3400\n",
            "Saving ...\n",
            "\n",
            "Epoch 6:\n",
            "500\n",
            "Training loss: 0.1436, Training accuracy: 0.9740\n",
            "Validation loss: 2.9415, Validation accuracy: 0.3802\n",
            "Saving ...\n",
            "\n",
            "Epoch 7:\n",
            "500\n",
            "Training loss: 0.0978, Training accuracy: 0.9900\n",
            "Validation loss: 2.1823, Validation accuracy: 0.4740\n",
            "Saving ...\n",
            "\n",
            "Epoch 8:\n",
            "500\n",
            "Training loss: 0.0577, Training accuracy: 0.9900\n",
            "Validation loss: 3.8373, Validation accuracy: 0.3861\n",
            "\n",
            "Epoch 9:\n",
            "500\n",
            "Training loss: 0.0533, Training accuracy: 0.9940\n",
            "Validation loss: 4.2742, Validation accuracy: 0.3138\n",
            "\n",
            "Epoch 10:\n",
            "500\n",
            "Training loss: 0.0374, Training accuracy: 0.9960\n",
            "Validation loss: 3.8677, Validation accuracy: 0.3558\n",
            "\n",
            "Epoch 11:\n",
            "500\n",
            "Training loss: 0.0211, Training accuracy: 1.0000\n",
            "Validation loss: 2.6379, Validation accuracy: 0.4404\n",
            "\n",
            "Epoch 12:\n",
            "500\n",
            "Training loss: 0.0123, Training accuracy: 1.0000\n",
            "Validation loss: 1.7675, Validation accuracy: 0.5321\n",
            "Saving ...\n",
            "\n",
            "Epoch 13:\n",
            "500\n",
            "Training loss: 0.0092, Training accuracy: 1.0000\n",
            "Validation loss: 1.6780, Validation accuracy: 0.5413\n",
            "Saving ...\n",
            "\n",
            "Epoch 14:\n",
            "500\n",
            "Training loss: 0.0057, Training accuracy: 1.0000\n",
            "Validation loss: 1.6461, Validation accuracy: 0.5383\n",
            "\n",
            "Current learning rate has decayed to 0.020000\n",
            "Epoch 15:\n",
            "500\n",
            "Training loss: 0.0048, Training accuracy: 1.0000\n",
            "Validation loss: 1.5681, Validation accuracy: 0.5530\n",
            "Saving ...\n",
            "\n",
            "Epoch 16:\n",
            "500\n",
            "Training loss: 0.0046, Training accuracy: 1.0000\n",
            "Validation loss: 1.5182, Validation accuracy: 0.5635\n",
            "Saving ...\n",
            "\n",
            "Epoch 17:\n",
            "500\n",
            "Training loss: 0.0042, Training accuracy: 1.0000\n",
            "Validation loss: 1.4985, Validation accuracy: 0.5689\n",
            "Saving ...\n",
            "\n",
            "Epoch 18:\n",
            "500\n",
            "Training loss: 0.0027, Training accuracy: 1.0000\n",
            "Validation loss: 1.4849, Validation accuracy: 0.5737\n",
            "Saving ...\n",
            "\n",
            "Epoch 19:\n",
            "500\n",
            "Training loss: 0.0037, Training accuracy: 1.0000\n",
            "Validation loss: 1.4754, Validation accuracy: 0.5743\n",
            "Saving ...\n",
            "\n",
            "Epoch 20:\n",
            "500\n",
            "Training loss: 0.0027, Training accuracy: 1.0000\n",
            "Validation loss: 1.4650, Validation accuracy: 0.5755\n",
            "Saving ...\n",
            "\n",
            "Epoch 21:\n",
            "500\n",
            "Training loss: 0.0030, Training accuracy: 1.0000\n",
            "Validation loss: 1.4554, Validation accuracy: 0.5769\n",
            "Saving ...\n",
            "\n",
            "Epoch 22:\n",
            "500\n",
            "Training loss: 0.0024, Training accuracy: 1.0000\n",
            "Validation loss: 1.4492, Validation accuracy: 0.5766\n",
            "\n",
            "Epoch 23:\n",
            "500\n",
            "Training loss: 0.0024, Training accuracy: 1.0000\n",
            "Validation loss: 1.4451, Validation accuracy: 0.5783\n",
            "Saving ...\n",
            "\n",
            "Epoch 24:\n",
            "500\n",
            "Training loss: 0.0020, Training accuracy: 1.0000\n",
            "Validation loss: 1.4409, Validation accuracy: 0.5799\n",
            "Saving ...\n",
            "\n",
            "Epoch 25:\n",
            "500\n",
            "Training loss: 0.0021, Training accuracy: 1.0000\n",
            "Validation loss: 1.4391, Validation accuracy: 0.5799\n",
            "\n",
            "Epoch 26:\n",
            "500\n",
            "Training loss: 0.0020, Training accuracy: 1.0000\n",
            "Validation loss: 1.4362, Validation accuracy: 0.5800\n",
            "Saving ...\n",
            "\n",
            "Epoch 27:\n",
            "500\n",
            "Training loss: 0.0023, Training accuracy: 1.0000\n",
            "Validation loss: 1.4346, Validation accuracy: 0.5802\n",
            "Saving ...\n",
            "\n",
            "Epoch 28:\n",
            "500\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 1.4306, Validation accuracy: 0.5806\n",
            "Saving ...\n",
            "\n",
            "Epoch 29:\n",
            "500\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 1.4273, Validation accuracy: 0.5815\n",
            "Saving ...\n",
            "\n",
            "Current learning rate has decayed to 0.004000\n",
            "Epoch 30:\n",
            "500\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 1.4277, Validation accuracy: 0.5822\n",
            "Saving ...\n",
            "\n",
            "Epoch 31:\n",
            "500\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 1.4270, Validation accuracy: 0.5826\n",
            "Saving ...\n",
            "\n",
            "Epoch 32:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4263, Validation accuracy: 0.5833\n",
            "Saving ...\n",
            "\n",
            "Epoch 33:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4250, Validation accuracy: 0.5842\n",
            "Saving ...\n",
            "\n",
            "Epoch 34:\n",
            "500\n",
            "Training loss: 0.0021, Training accuracy: 1.0000\n",
            "Validation loss: 1.4260, Validation accuracy: 0.5843\n",
            "Saving ...\n",
            "\n",
            "Epoch 35:\n",
            "500\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 1.4257, Validation accuracy: 0.5849\n",
            "Saving ...\n",
            "\n",
            "Epoch 36:\n",
            "500\n",
            "Training loss: 0.0020, Training accuracy: 1.0000\n",
            "Validation loss: 1.4254, Validation accuracy: 0.5839\n",
            "\n",
            "Epoch 37:\n",
            "500\n",
            "Training loss: 0.0021, Training accuracy: 1.0000\n",
            "Validation loss: 1.4263, Validation accuracy: 0.5843\n",
            "\n",
            "Epoch 38:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4258, Validation accuracy: 0.5839\n",
            "\n",
            "Epoch 39:\n",
            "500\n",
            "Training loss: 0.0021, Training accuracy: 1.0000\n",
            "Validation loss: 1.4247, Validation accuracy: 0.5843\n",
            "\n",
            "Current learning rate has decayed to 0.000800\n",
            "Epoch 40:\n",
            "500\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 1.4242, Validation accuracy: 0.5846\n",
            "\n",
            "Epoch 41:\n",
            "500\n",
            "Training loss: 0.0023, Training accuracy: 1.0000\n",
            "Validation loss: 1.4264, Validation accuracy: 0.5840\n",
            "\n",
            "Epoch 42:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4271, Validation accuracy: 0.5842\n",
            "\n",
            "Epoch 43:\n",
            "500\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 1.4260, Validation accuracy: 0.5843\n",
            "\n",
            "Epoch 44:\n",
            "500\n",
            "Training loss: 0.0019, Training accuracy: 1.0000\n",
            "Validation loss: 1.4266, Validation accuracy: 0.5839\n",
            "\n",
            "Epoch 45:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4257, Validation accuracy: 0.5839\n",
            "\n",
            "Epoch 46:\n",
            "500\n",
            "Training loss: 0.0017, Training accuracy: 1.0000\n",
            "Validation loss: 1.4254, Validation accuracy: 0.5845\n",
            "\n",
            "Epoch 47:\n",
            "500\n",
            "Training loss: 0.0016, Training accuracy: 1.0000\n",
            "Validation loss: 1.4246, Validation accuracy: 0.5844\n",
            "\n",
            "Epoch 48:\n",
            "500\n",
            "Training loss: 0.0016, Training accuracy: 1.0000\n",
            "Validation loss: 1.4241, Validation accuracy: 0.5839\n",
            "\n",
            "Epoch 49:\n",
            "500\n",
            "Training loss: 0.0018, Training accuracy: 1.0000\n",
            "Validation loss: 1.4248, Validation accuracy: 0.5846\n",
            "\n",
            "==================================================\n",
            "==> Optimization finished! Best validation accuracy: 0.5849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w79iPLjtgj_Q"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Credit goes to https://github.com/p3i0t/SimCLR-CIFAR10\n",
        "'''\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimCLR(nn.Module):\n",
        "    def __init__(self, base_encoder, projection_dim=128):\n",
        "        super().__init__()\n",
        "        self.enc = base_encoder(pretrained=False)  # load model from torchvision.models without pretrained weights.\n",
        "        self.feature_dim = self.enc.fc.in_features\n",
        "\n",
        "        # Customize for CIFAR10. Replace conv 7x7 with conv 3x3, and remove first max pooling.\n",
        "        # See Section B.9 of SimCLR paper.\n",
        "        self.enc.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        self.enc.maxpool = nn.Identity()\n",
        "        self.enc.fc = nn.Identity()  # remove final fully connected layer.\n",
        "\n",
        "        # Add MLP projection.\n",
        "        self.projection_dim = projection_dim\n",
        "        self.projector = nn.Sequential(nn.Linear(self.feature_dim, 2048),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Linear(2048, projection_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.enc(x)\n",
        "        projection = self.projector(feature)\n",
        "        return feature, projection\n",
        "\n",
        "def simclr_resnet50():\n",
        "    return SimCLR(torchvision.models.resnet50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e56dc15573c446b6ac8c3e2dfddf619c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73a99ccd039c4288947bdb7f2e6e76d0",
              "IPY_MODEL_154d79d59e944a88874c128d11d82916",
              "IPY_MODEL_3bc62df8b4b140a3a5f49977495fa6aa"
            ],
            "layout": "IPY_MODEL_ede04e760e2446a59d7dd0a74a803182"
          }
        },
        "73a99ccd039c4288947bdb7f2e6e76d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6a99f944034a9eab12d99a4b9368c6",
            "placeholder": "​",
            "style": "IPY_MODEL_706a00ed84ef49b0b29893369cce5f44",
            "value": "100%"
          }
        },
        "154d79d59e944a88874c128d11d82916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e37db0dc5f5f43eca04a53b389edb6ef",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67c4fa30285a4ac6a2363f72b4175d89",
            "value": 170498071
          }
        },
        "3bc62df8b4b140a3a5f49977495fa6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9cb320b2aab4d8ca814c5ccd456eeff",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc9055552324dd0b0585682ff8855be",
            "value": " 170498071/170498071 [00:02&lt;00:00, 85724800.91it/s]"
          }
        },
        "ede04e760e2446a59d7dd0a74a803182": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6a99f944034a9eab12d99a4b9368c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706a00ed84ef49b0b29893369cce5f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e37db0dc5f5f43eca04a53b389edb6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c4fa30285a4ac6a2363f72b4175d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9cb320b2aab4d8ca814c5ccd456eeff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc9055552324dd0b0585682ff8855be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}